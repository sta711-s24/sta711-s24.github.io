---
title: "Lecture 4: Maximum likelihood estimation"
format: 
  revealjs:
    theme: theme.scss
editor: source
editor_options: 
  chunk_output_type: console
---

## Reminders

* HW 1 due Friday 11am on Canvas
* You are always welcome to work together on assignments, just make sure to write up your own solutions
* You have a bank of 5 extension days; you can use 1 or 2 on any assignment or exam

## Recap: maximum likelihood estimation

**Definition:** Let ${\bf Y} = (Y_1,...,Y_n)$ be a sample of $n$ observations, and let $f({\bf y}|\theta)$ denote the joint pdf or pmf of ${\bf Y}$, with parameter(s) $\theta$. The *likelihood function* is
$$L(\theta | {\bf Y}) = f({\bf Y} | \theta)$$

**Definition:** Let ${\bf Y} = (Y_1,...,Y_n)$ be a sample of $n$ observations. The *maximum likelihood estimator* (MLE) is 

$$\widehat{\theta} = \ \text{argmax}_{\theta} \ L(\theta | {\bf Y})$$


## Example: $N(\theta, 1)$


## Example: $Uniform(0, \theta)$

Let $Y_1,...,Y_n \overset{iid}{\sim} Uniform(0, \theta)$, where $\theta > 0$. We want the maximum likelihood estimator of $\theta$.

Discuss with your neighbors what the MLE of $\theta$ might be. *Hint: focus on finding and sketching the likelihood function* $L({\bf Y}|\theta)$


## Example: $N(\mu, \sigma^2)$


## Logistic regression

$$Y_i \sim Bernoulli(p_i)$$

$$\log \left( \dfrac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_{i,1} + \cdots + \beta_k X_{i,k}$$

Suppose we observe independent samples $(X_1, Y_1),...,(X_n, Y_n)$. Write down the likelihood function

$$L(\beta | {\bf X}, {\bf Y}) = \prod \limits_{i=1}^n f(Y_i| \beta, X_i)$$

for the logistic regression problem.
