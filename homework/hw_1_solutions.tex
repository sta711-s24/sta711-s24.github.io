\documentclass[12pt]{article}

% Fonts and typesetting
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{newtxtext}
\usepackage{newtxmath}
\usepackage{microtype}
\usepackage{booktabs}

% More generous page layout
\usepackage{geometry}

\usepackage{graphicx}
\usepackage{hyperref}

\usepackage{amsmath}
%\usepackage{amssymb}

\usepackage{float}

\title{STA 711 HW 1 Solutions}

\author{Ciaran Evans}
\date{}

\begin{document}

\maketitle

\begin{enumerate}
\item Suppose that $X \sim Poisson(\lambda)$.

\begin{enumerate}
\item We use the fact that $e^\lambda = \sum \limits_{x=0}^{\infty} \dfrac{\lambda^{x}}{x!}$.

\begin{align*}
\mathbb{E}[X] &= \sum \limits_{x=0}^{\infty} x \frac{\lambda^x e^{-\lambda}}{x!} = \sum \limits_{x=1}^{\infty} x \frac{\lambda^x e^{-\lambda}}{x!} \\ 
&= e^{-\lambda} \lambda \sum \limits_{x=1}^{\infty} \frac{\lambda^{x-1}}{(x-1)!} = e^{-\lambda} \lambda \sum \limits_{x=0}^{\infty} \frac{\lambda^{x}}{x!} \\
&= e^{-\lambda} \lambda e^{\lambda} \\
&= \lambda
\end{align*}

\begin{align*}
\mathbb{E}[X^2] &= \sum \limits_{x=0}^{\infty} x^2 \frac{\lambda^x e^{-\lambda}}{x!} = \sum \limits_{x=1}^{\infty} x^2 \frac{\lambda^x e^{-\lambda}}{x!} \\
&= \lambda e^{-\lambda} \sum \limits_{x=1}^{\infty} x^2 \frac{\lambda^{x-1}}{x!} \\
&= \lambda e^{-\lambda} \sum \limits_{x=1}^{\infty} x \frac{\lambda^{x-1}}{(x-1)!} \\
&= \lambda e^{-\lambda} \left( \sum \limits_{x=1}^{\infty} (x-1) \frac{\lambda^{x-1}}{(x-1)!} +  \sum \limits_{x=1}^{\infty} \frac{\lambda^{x-1}}{(x-1)!}\right) \\
&= \lambda e^{-\lambda} \left( \lambda e^{\lambda} + e^{\lambda} \right) \\
&= \lambda^2 + \lambda
\end{align*}

Thus, $Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X] = \lambda$.

\item 

\begin{align*}
\mathbb{E}[e^{tX}] = \sum \limits_{x=0}^{\infty} e^{tx} \frac{\lambda^x e^{-\lambda}}{x!} = e^{-\lambda} \sum \limits_{x=0}^\infty \frac{(\lambda e^t)^x}{x!} = e^{-\lambda} e^{\lambda e^t} = \exp\{\lambda (e^t - 1)\}
\end{align*}

\item From (b), $M_X(t) = \exp\{\lambda (e^t - 1)\}$, so
\begin{align*}
M'_X(t) = \exp\{\lambda (e^t - 1)\} \lambda e^t
\end{align*}
and
\begin{align*}
M''_X(t) = \exp\{\lambda (e^t - 1)\} \lambda^2 e^{2t} + \exp\{\lambda (e^t - 1)\} \lambda
\end{align*}
At $t = 0$, we get
\begin{align*}
\mathbb{E}[X] &= M_X'(0) = \lambda \\
\mathbb{E}[X^2] &= M_X''(0) = \lambda^2 + \lambda
\end{align*}
and thus $Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X] = \lambda$.

\end{enumerate}

\item Suppose that $X \sim Poisson(\lambda)$ and $Y \sim Poisson(\mu)$ are independent. Let $Z = X + Y$.

\begin{enumerate}
\item We use the fact that the MGF for the sum of independent random variables is the product of their individual MGFs:
\begin{align*}
M_Z(t) &= M_X(t)M_Y(t) = \exp\{\lambda (e^t - 1)\}\exp\{\mu (e^t - 1)\} \\
&= \exp\{(\lambda + \mu) (e^t - 1)\}
\end{align*}
This is the MGF of a $Poisson(\lambda + \mu)$ distribution, and so by the uniqueness of MGFs we conclude $X + Y \sim Poisson(\lambda + \mu)$.

\item 

\begin{align*}
P(X = x | Z = n) &= \frac{P(X = x, Z = n)}{P(Z = n)} = \frac{P(X = x, Y = n - x)}{P(Z = n)} \\
&= \dfrac{\dfrac{\lambda^x e^{-\lambda}}{x!} \cdot \dfrac{\mu^{n-x} e^{-\mu}}{(n-x)!}}{\dfrac{(\lambda + \mu)^n e^{-(\lambda + \mu)}}{n!}} \\
&= \frac{n!}{x!(n - x)!} \left( \frac{\lambda}{\lambda + \mu} \right)^x \left( 1 - \frac{\lambda}{\lambda + \mu} \right)^{n-x}
\end{align*}

This is the pmf of a $Binomial\left(n, \frac{\lambda}{\lambda + \mu} \right)$ distribution.

\end{enumerate}


\item The joint pdf is

$$f(x,y) = \begin{cases} 
c(x + 2y) & 0 < y < 1, 0 < x < 2 \\
0 & \text{else}
\end{cases}$$

\begin{enumerate}

\item To find $c$, we note that $f(x,y)$ must integrate to 1.

\begin{align*}
c \int \limits_0^1 \int \limits_0^2 (x + 2y) dx dy &= c \int \limits_0^1 \int \limits_0^2 x dx dy + c \int \limits_0^1 \int \limits_0^2 2y dx dy \\
&= 2c + 2c
\end{align*}

so we conclude that $c= 1/4$.

\item 

\begin{align*}
f(x) = \int \limits_0^1 f(x,y) dy = \frac{1}{4} \int \limits_0^1 (x + 2y) dy = \frac{x + 1}{4}
\end{align*}

\item 

\begin{align*}
P(Z < t) &= P(9 \leq (1 + X)^2t) = P(\frac{3}{\sqrt{t}} - 1 \leq X) = \int \limits_{3/\sqrt{t} - 1}^1 (x + 1)/4 dx \\
&= \frac{1}{2} - \frac{9}{8t}
\end{align*}

Then, differentiating,
\begin{align*}
f_Z(t) = \frac{9}{8 t^2}
\end{align*}
for $t \in [9/4, 9]$

\end{enumerate}

\item 

\begin{enumerate}
\item

\begin{align*}
\mathbb{E}[Y] &= \int \limits_0^\infty \frac{1}{\Gamma(k) \theta^k} y^k e^{-y/\theta} dy \\
&= \frac{1}{\Gamma(k) \theta^k} \int \limits_0^\infty y^k e^{-y/\theta} dy
\end{align*}
Now, note that $y^k e^{-y/\theta}$ is the kernel of a $Gamma(k+1, \theta)$ distribution, and so we must have
\begin{align*}
\int \limits_0^\infty y^k e^{-y/\theta} dy = \theta^{k+1} \Gamma(k+1)
\end{align*}
Therefore,
\begin{align*}
\mathbb{E}[Y] = \theta \frac{\Gamma(k+1)}{\Gamma(k)}
\end{align*}
It then only remains to show that $\frac{\Gamma(k+1)}{\Gamma(k)} = k$. We will use integration by parts; recall that $\int u dv = uv - \int v du$. Then, with $u = y^k$ and $v = -e^{-y}$,
\begin{align*}
\Gamma(k+1) = \int \limits_0^\infty y^k e^{-y} dy = -y^k e^{-y} \biggr\lvert_0^\infty + \int \limits_0^\infty e^{-y} ky^{k-1} dy = k \Gamma(k)
\end{align*}

\item 

\begin{align*}
\mathbb{E}[Y^2] = \frac{1}{\Gamma(k)\theta^k} \int \limits_0^\infty y^{k+1} e{-y/\theta} dy = \frac{\theta^{k+2} \Gamma(k+2)}{\theta^k \Gamma(k)} = \theta^2 \frac{\Gamma(k+2)}{\Gamma(k)} = \theta^2 (k+1)k,
\end{align*}
where similar to part (a), we have recognized $y^{k+1} e{-y/\theta}$ as the kernel of a $Gamma(k+2, \theta)$ distribution. Then,

\begin{align*}
Var(Y) = \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 = \theta^2k^2 - \theta^2k - \theta^2k^2 = k\theta^2
\end{align*}

\item 

\begin{align*}
M_Y(t) = \mathbb{E}[e^{tY}] &= \frac{1}{\Gamma(k) \theta^k} \int \limits_0^\infty e^{ty} y^{k-1} e^{-y/\theta} dy \\
&= \frac{1}{\Gamma(k) \theta^k} \int \limits_0^\infty y^{k-1} \exp\left\lbrace -\left(\frac{1}{\theta} - t\right)y \right\rbrace dy \\
&= \frac{1}{\Gamma(k) \theta^k} \cdot \Gamma(k) \left(\frac{1}{\theta} - t\right)^{-k},
\end{align*}
recognizing $y^{k-1} \exp\left\lbrace -\left(\frac{1}{\theta} - t\right)y \right\rbrace$ as the kernel of a Gamma distribution. Simplifying,
\begin{align*}
M_Y(t) = (1 - \theta t)^{-k}
\end{align*}

\item Using part (c), the mgf for one of the variables is $M(t) = \left(1 - 2t \right)^{-1/2}$. Since we have a sum of independent variables, the mgf for the sum is
\begin{align*}
M_{\sum_i Y_i}(t) = \prod \limits_i \left(1 - 2t \right)^{-1/2} = \left(1 - 2t \right)^{-n/2}
\end{align*}
which is the mgf for a $Gamma(n/2, 2)$ distribution (also known as a $\chi^2_n$ distribution!)

\end{enumerate}

\item 

\begin{enumerate}
\item 

\begin{align*}
\mathbb{E}[Y] = \mathbb{E}[\mathbb{E}[Y|\lambda]] = \mathbb{E}[\lambda] = \frac{1}{\psi} \cdot \mu \psi = \mu,
\end{align*}
using the fact that $\lambda$ follows a Gamma distribution, and the mean of a Gamma distribution calculated in 4(a).

\item

\begin{align*}
Var(Y) &= \mathbb{E}[Var(Y|\lambda)] + Var(\mathbb{E}[Y|\lambda]) \\
&= \mathbb{E}[\lambda] + Var(\lambda) \\
&= \mu + \mu^2 \psi
\end{align*}

\item (It helps to use the fact that $\Gamma(y+1) = y!$)

\begin{align*}
P(Y = y) &= \int \limits_0^\infty P(Y = y | \lambda) f(\lambda) d \lambda \\
&= \int \limits_0^\infty \frac{\lambda^y e^{-\lambda}}{y!} \cdot \frac{\lambda^{\frac{1}{\psi} - 1}e^{\frac{-\lambda}{\mu \psi}}}{\Gamma(1/\psi)(\mu \psi)^{1/\psi}} d \lambda \\
&= \int \limits_0^\infty \frac{\lambda^y e^{-\lambda}}{\Gamma(y+1)} \cdot \frac{\lambda^{\frac{1}{\psi} - 1}e^{\frac{-\lambda}{\mu \psi}}}{\Gamma(1/\psi)(\mu \psi)^{1/\psi}} d \lambda \\
\end{align*}


\end{enumerate}

\item 

\begin{enumerate}
\item Since $F_X$ is a monotonic, continuous cdf, then $F_X$ and $F_X^{-1}$ are both strictly increasing, and
\begin{align*}
P(X \leq t) = P(F_X^{-1}(U) \leq t) = P(U \leq F_X(t)) = F_X(t),
\end{align*}
where the final step follows from the cdf of a $Uniform(0,1)$ distribution.

\item See the R part below.

\end{enumerate}

\item 

\begin{enumerate}
\item

\begin{align*}
Cov\left( \sum \limits_{i=1}^m a_i X_i, \sum \limits_{j=1}^n b_j Y_j \right) &= \mathbb{E}\left[ \left( \sum \limits_{i=1}^m a_i X_i - \sum \limits_{i=1}^m a_i \mathbb{E}[X_i] \right)\left(\sum \limits_{j=1}^n b_j Y_j - \sum \limits_{j=1}^n b_j \mathbb{E}[Y_j] \right) \right] \\
&= \mathbb{E}\left[ \left( \sum \limits_{i=1}^m a_i (X_i - \mathbb{E}[X_i])  \right)\left(\sum \limits_{j=1}^n b_j (Y_j - \mathbb{E}[Y_j]) \right) \right] \\
&= \mathbb{E}\left[ \sum \limits_{i=1}^m \sum \limits_{j=1}^n a_i b_j  (X_i - \mathbb{E}[X_i])(Y_j - \mathbb{E}[Y_j]) \right] \\
&= \sum \limits_{i=1}^m \sum \limits_{j=1}^n a_i b_j  \mathbb{E}\left[ (X_i - \mathbb{E}[X_i])(Y_j - \mathbb{E}[Y_j])\right] \\
&= \sum \limits_{i=1}^m \sum \limits_{j=1}^n a_i b_j  Cov(X_i, Y_j)
\end{align*}

\item 

\begin{align*}
Var \left( \sum \limits_{i=1}^m X_i \right) &= Cov \left( \sum \limits_{i=1}^m X_i, \sum \limits_{i=1}^m X_i \right) \\ 
&= \sum \limits_{i=1}^m \sum \limits_{j=1}^m Cov(X_i, X_j) \\
&= \sum \limits_{i=1}^m Var(X_i) + \sum \limits_{i=1}^m \sum \limits_{j \neq i} Cov(X_i, X_j) \\
&= \sum \limits_{i=1}^m Var(X_i) + 2 \sum \limits_{i < j} Cov(X_i, X_j)
\end{align*}
\end{enumerate}

\item $X_1,...,X_n \overset{iid}{\sim} Uniform(0, 1)$, and $Y = \max\{X_1,...,X_n\}$

\begin{enumerate}

\item For $t \in [0, 1]$:

\begin{align*}
P(Y \leq t) &= P(X_1,...,X_n \leq t) = \prod \limits_{i=1}^n P(X_i \leq t) \text{ (independence) } \\
&= t^n
\end{align*}
Then, the pdf is 
\begin{align*}
f_Y(t) = nt^{n-1}
\end{align*}

\item 

\begin{align*}
\mathbb{E}[Y] = \int \limits_{0}^{1} t \cdot n t^{n-1} dt = \frac{n}{n+1} t^{n+1} \biggr\lvert_0^1 = \frac{n}{n+1}
\end{align*}

\item We want to find the pdf of $X_{(k)}$; we begin by finding the cdf, then differentiate.

To find $P(X_{(k)} \leq t)$, note that $X_{(k)} \leq t$ requires \textit{at least} $k$ of $X_1,...,X_n$ to be $\leq t$ -- and, possibly, all $n$ are $\leq t$. That is,

\begin{align*}
P(X_{(k)} \leq t) &= \sum \limits_{i=k}^n P(\text{exactly } i \text{ of } X_1,...,X_n \leq t) \\
&= \sum \limits_{i=k}^n { n \choose i }  F_X(t)^{i}(1 - F_X(t))^{n-i} \\
&= \sum \limits_{i=k}^n { n \choose i} t^{i}(1 - t)^{n-i} \ \text{ (Uniform distribution)}
\end{align*}

Differentiating,

\begin{align*}
f(t) &= \sum \limits_{i=k}^n \left[ {n \choose i} i t^{i-1}(1 - t)^{n-i} - {n \choose i} (n-i) t^i (1-t)^{n-i-1} \right] \\
&= {n \choose k} k t^{k-1}(1 - t)^{n-k} + \sum \limits_{i=k+1}^n{n \choose i} i t^{i-1}(1 - t)^{n-i} - \sum \limits_{i=k}^n {n \choose i} (n-i) t^i (1-t)^{n-i-1}
\end{align*}

Since $n - i = 0$ when $i = n$, then we have
\begin{align*}
f(t) &= {n \choose k} k t^{k-1}(1 - t)^{n-k} + \sum \limits_{i=k+1}^n{n \choose i} i t^{i-1}(1 - t)^{n-i} - \sum \limits_{i=k}^{n-1} {n \choose i} (n-i) t^i (1-t)^{n-i-1} \\
&= {n \choose k} k t^{k-1}(1 - t)^{n-k} + \sum \limits_{i=k}^{n-1}{n \choose i+1} (i+1) t^{i}(1 - t)^{n-i-1} - \sum \limits_{i=k}^{n-1} {n \choose i} (n-i) t^i (1-t)^{n-i-1}
\end{align*}
Since ${n \choose i+1} (i+1) = {n \choose i} (n-i)$, the second and third terms cancel out, and we are left with
\begin{align*}
f(t) = {n \choose k} k t^{k-1}(1 - t)^{n-k}
\end{align*}

\end{enumerate}

\end{enumerate}

\end{document}